{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, Input, Add, Concatenate, GlobalAveragePooling2D, Dense, Reshape\n",
    "from tensorflow.keras.layers import ZeroPadding2D,MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "dataset_dir = '/kaggle/input/ziptrain/mini proj'\n",
    "dataset_dir2 = '/kaggle/input/ziptest/validation set'\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    dataset_dir,            \n",
    "    target_size=(224, 224),  \n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    dataset_dir2,            \n",
    "    target_size=(224, 224),  \n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "def identity_block(X, f, filters, stage, block):\n",
    "   \n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = ReLU()(X)\n",
    "\n",
    "    # Second component of main path\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = ReLU()(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a ReLU activation\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = ReLU()(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = ReLU()(X)\n",
    "\n",
    "    # Second component of main path\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1, 1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = ReLU()(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    ##### SHORTCUT PATH ####\n",
    "    X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s, s), padding = 'valid', name = conv_name_base + '1',\n",
    "                        kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a ReLU activation\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = ReLU()(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def ResNet50(input_shape=(224, 224, 3), classes=1000):\n",
    "   \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = ReLU()(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    # Stage 3\n",
    "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    # Stage 4\n",
    "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    # Stage 5\n",
    "    X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    # AVGPOOL\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "\n",
    "    # output layer\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = ResNet50(input_shape=(224, 224, 3), classes=3)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer1 = Adam(learning_rate=0.001)\n",
    "optimizer2 = SGD(learning_rate=0.01)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer1, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in tqdm(range(10), desc=\"Training Epochs\"):\n",
    "    history = model.fit(train_generator, epochs=1, validation_data=validation_generator)\n",
    "    \n",
    "    train_accs.append(history.history['accuracy'][0])\n",
    "    train_losses.append(history.history['loss'][0])\n",
    "    val_accs.append(history.history['val_accuracy'][0])\n",
    "    val_losses.append(history.history['val_loss'][0])\n",
    "\n",
    "\n",
    "# Compile the model with the second optimizer\n",
    "model.compile(optimizer=optimizer2, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with the second optimizer\n",
    "for epoch in tqdm(range(90), desc=\"Training Epochs\"):\n",
    "    history2 = model.fit(train_generator, epochs=1, validation_data=validation_generator)\n",
    "    \n",
    "    train_accs.append(history2.history['accuracy'][0])\n",
    "    train_losses.append(history2.history['loss'][0])\n",
    "    val_accs.append(history2.history['val_accuracy'][0])\n",
    "    val_losses.append(history2.history['val_loss'][0])\n",
    "\n",
    "# Save the model\n",
    "model.save('resnet50_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T05:14:07.528531Z",
     "iopub.status.busy": "2024-04-29T05:14:07.528190Z",
     "iopub.status.idle": "2024-04-29T05:14:11.642921Z",
     "shell.execute_reply": "2024-04-29T05:14:11.642022Z",
     "shell.execute_reply.started": "2024-04-29T05:14:07.528507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "[[3.1504896e-04 9.9967653e-01 8.3557670e-06]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "loaded_model = tf.keras.models.load_model('resnet50_model.h5')\n",
    "your_image_path = \"/kaggle/input/prostate-pred/0004.png\"\n",
    "img = image.load_img(your_image_path, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Convert single image to batch of size 1\n",
    "\n",
    "# Preprocess the image\n",
    "img_array = img_array / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(img_array)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4777407,
     "sourceId": 8091817,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4777411,
     "sourceId": 8091822,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4777421,
     "sourceId": 8091838,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4818835,
     "sourceId": 8148370,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4846620,
     "sourceId": 8185213,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
